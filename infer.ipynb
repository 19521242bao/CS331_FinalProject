{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T03:25:11.304478Z",
     "iopub.status.busy": "2021-10-20T03:25:11.303989Z",
     "iopub.status.idle": "2021-10-20T03:26:34.978937Z",
     "shell.execute_reply": "2021-10-20T03:26:34.978099Z",
     "shell.execute_reply.started": "2021-10-20T03:25:11.304442Z"
    }
   },
   "outputs": [],
   "source": [
    "## Optional if you already run ./down_viecap.sh\n",
    "# !pip install gdown\n",
    "# !gdown https://drive.google.com/uc?id=1lbOTlksNA5a97_Ydqh84TE6Dm85Rsy60\n",
    "# !gdown https://drive.google.com/uc?id=1lKqdtekrhzlf7duVs34IQPVLROowoLoR\n",
    "# !gdown https://drive.google.com/uc?id=1rCDniCZNgaJ7WQUzPpzEwuXW5_WNyave\n",
    "# !unzip viecap4h-public-train.zip\n",
    "# !unzip vietcap4h-public-test.zip\n",
    "# !unzip -qq viecap4h-public-train/images_train.zip\n",
    "# !unzip -qq vietcap4h-public-test/images_public_test.zip\n",
    "# !unzip -qq vietcap4h-private-test.zip\n",
    "# !mkdir viecap\n",
    "# !mv images_train viecap\n",
    "# !mv images_public_test viecap\n",
    "# !mv vietcap4h-private-test/images viecap/images_private_test\n",
    "# !mv vietcap4h-private-test/private_sample_sub.json viecap\n",
    "# !mv viecap4h-public-train/train_captions.json viecap\n",
    "# !mv vietcap4h-public-test/sample_submission.json viecap\n",
    "# !rm *.zip\n",
    "# !rm -rf viecap4h-public-train/\n",
    "# !rm -rf vietcap4h-public-test/\n",
    "# !pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T03:26:34.986466Z",
     "iopub.status.busy": "2021-10-20T03:26:34.984348Z",
     "iopub.status.idle": "2021-10-20T03:26:58.911146Z",
     "shell.execute_reply": "2021-10-20T03:26:58.910359Z",
     "shell.execute_reply.started": "2021-10-20T03:26:34.986423Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 335M/335M [00:03<00:00, 112MiB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdd8aedefcd44539465647c3a696713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e4fa3bdfa449138e4ab53c81b2227e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import os\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nnf\n",
    "import sys\n",
    "from typing import Tuple, List, Union, Optional\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import skimage.io as io\n",
    "import PIL.Image\n",
    "from IPython.display import Image \n",
    "\n",
    "\n",
    "N = type(None)\n",
    "V = np.array\n",
    "ARRAY = np.ndarray\n",
    "ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n",
    "VS = Union[Tuple[V, ...], List[V]]\n",
    "VN = Union[V, N]\n",
    "VNS = Union[VS, N]\n",
    "T = torch.Tensor\n",
    "TS = Union[Tuple[T, ...], List[T]]\n",
    "TN = Optional[T]\n",
    "TNS = Union[Tuple[TN, ...], List[TN]]\n",
    "TSN = Optional[TS]\n",
    "TA = Union[T, ARRAY]\n",
    "\n",
    "\n",
    "D = torch.device\n",
    "CPU = torch.device('cpu')\n",
    "\n",
    "\n",
    "def get_device(device_id: int) -> D:\n",
    "    if not torch.cuda.is_available():\n",
    "        return CPU\n",
    "    device_id = min(torch.cuda.device_count() - 1, device_id)\n",
    "    return torch.device(f'cuda:{device_id}')\n",
    "\n",
    "\n",
    "CUDA = get_device(0)\n",
    "\n",
    "# current_directory = os.getcwd()\n",
    "# save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n",
    "# os.makedirs(save_path, exist_ok=True)\n",
    "# model_path = os.path.join(save_path, 'model_wieghts.pt')\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def forward(self, x: T) -> T:\n",
    "        return self.model(x)\n",
    "\n",
    "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        for i in range(len(sizes) -1):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
    "            if i < len(sizes) - 2:\n",
    "                layers.append(act())\n",
    "#                 layers.append(nn.Dropout(p=0.1))\n",
    "                layers.append(nn.Dropout(p=0.2)) # update 17/10 20:00PM\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ClipCaptionModel(nn.Module):\n",
    "\n",
    "    #@functools.lru_cache #FIXME\n",
    "    def get_dummy_token(self, batch_size: int, device: D) -> T:\n",
    "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
    "\n",
    "    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n",
    "        embedding_text = self.gpt.transformer.wte(tokens)\n",
    "        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n",
    "        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n",
    "        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n",
    "        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n",
    "        if labels is not None:\n",
    "            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n",
    "            labels = torch.cat((dummy_token, tokens), dim=1)\n",
    "        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n",
    "        return out\n",
    "\n",
    "    def __init__(self, prefix_length: int, prefix_size: int = 512):\n",
    "        super(ClipCaptionModel, self).__init__()\n",
    "        self.prefix_length = prefix_length\n",
    "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        if prefix_length > 10:  # not enough memory\n",
    "            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n",
    "        else:\n",
    "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n",
    "\n",
    "\n",
    "class ClipCaptionPrefix(ClipCaptionModel):\n",
    "\n",
    "    def parameters(self, recurse: bool = True):\n",
    "        return self.clip_project.parameters()\n",
    "\n",
    "    def train(self, mode: bool = True):\n",
    "        super(ClipCaptionPrefix, self).train(mode)\n",
    "        self.gpt.eval()\n",
    "        return self\n",
    "    \n",
    "\n",
    "def generate2(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        tokens=None,\n",
    "        prompt=None,\n",
    "        embed=None,\n",
    "        entry_count=1,\n",
    "        entry_length=67,  # maximum number of words\n",
    "        top_p=0.8,\n",
    "        temperature=1.,\n",
    "        stop_token: str = '.',\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "#     stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    stop_token_index = 0\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in trange(entry_count):\n",
    "            if embed is not None:\n",
    "                generated = embed\n",
    "            else:\n",
    "                if tokens is None:\n",
    "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                    tokens = tokens.unsqueeze(0).to(device)\n",
    "\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "\n",
    "                outputs = model.gpt(inputs_embeds=generated)\n",
    "                logits = outputs.logits\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                                                    ..., :-1\n",
    "                                                    ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "                next_token_embed = model.gpt.transformer.wte(next_token)\n",
    "                if tokens is None:\n",
    "                    tokens = next_token\n",
    "                else:\n",
    "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "                if stop_token_index == next_token.item():\n",
    "                    break\n",
    "\n",
    "            output_list = list(tokens.squeeze().cpu().numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]\n",
    "\n",
    "#@title GPU/CPU\n",
    "\n",
    "is_gpu = True #@param {type:\"boolean\"}  \n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#@title CLIP model + GPT2 tokenizer\n",
    "\n",
    "device = CUDA\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=device, jit=False)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"imthanhlv/gpt2news\")\n",
    "\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T03:26:58.912981Z",
     "iopub.status.busy": "2021-10-20T03:26:58.912667Z",
     "iopub.status.idle": "2021-10-20T03:26:58.930118Z",
     "shell.execute_reply": "2021-10-20T03:26:58.929227Z",
     "shell.execute_reply.started": "2021-10-20T03:26:58.912941Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
    "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
    "\n",
    "    model.eval()\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "#     stop_token_index = 0\n",
    "\n",
    "    tokens = None\n",
    "    scores = None\n",
    "    device = next(model.parameters()).device\n",
    "    seq_lengths = torch.ones(beam_size, device=device)\n",
    "    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n",
    "    with torch.no_grad():\n",
    "        if embed is not None:\n",
    "            generated = embed\n",
    "        else:\n",
    "            if tokens is None:\n",
    "                tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                tokens = tokens.unsqueeze(0).to(device)\n",
    "                generated = model.gpt.transformer.wte(tokens)\n",
    "        for i in range(entry_length):\n",
    "            outputs = model.gpt(inputs_embeds=generated)\n",
    "            logits = outputs.logits\n",
    "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "            logits = logits.softmax(-1).log()\n",
    "            if scores is None:\n",
    "                scores, next_tokens = logits.topk(beam_size, -1)\n",
    "                generated = generated.expand(beam_size, *generated.shape[1:])\n",
    "                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n",
    "                if tokens is None:\n",
    "                    tokens = next_tokens\n",
    "                else:\n",
    "                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n",
    "                    tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "            else:\n",
    "                logits[is_stopped] = -float(np.inf)\n",
    "                logits[is_stopped, 0] = 0\n",
    "                scores_sum = scores[:, None] + logits\n",
    "                seq_lengths[~is_stopped] += 1\n",
    "                scores_sum_average = scores_sum / seq_lengths[:, None]\n",
    "                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n",
    "                next_tokens_source = next_tokens // scores_sum.shape[1]\n",
    "                seq_lengths = seq_lengths[next_tokens_source]\n",
    "                next_tokens = next_tokens % scores_sum.shape[1]\n",
    "                next_tokens = next_tokens.unsqueeze(1)\n",
    "                tokens = tokens[next_tokens_source]\n",
    "                tokens = torch.cat((tokens, next_tokens), dim=1)\n",
    "                generated = generated[next_tokens_source]\n",
    "                scores = scores_sum_average * seq_lengths\n",
    "                is_stopped = is_stopped[next_tokens_source]\n",
    "            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n",
    "            generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n",
    "            if is_stopped.all():\n",
    "                break\n",
    "    scores = scores / seq_lengths\n",
    "    output_list = tokens.cpu().numpy()\n",
    "    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n",
    "    order = scores.argsort(descending=True)\n",
    "    output_texts = [output_texts[i] for i in order]\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-20T03:28:29.812133Z",
     "iopub.status.busy": "2021-10-20T03:28:29.811753Z",
     "iopub.status.idle": "2021-10-20T03:29:04.597435Z",
     "shell.execute_reply": "2021-10-20T03:29:04.596629Z",
     "shell.execute_reply.started": "2021-10-20T03:28:29.812096Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f35760b6434a3eb09714157689a309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202a3f8d31a445828213acee2797a5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefix_length = 10\n",
    "\n",
    "model = ClipCaptionPrefix(prefix_length)\n",
    "model_path = \"./iwlts-nmt-015-x3b16.pt\"\n",
    "model.load_state_dict(torch.load(model_path, map_location=CPU)) \n",
    "model = model.eval() \n",
    "device =\"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_dir=\"../data/test_images\"# change image dir for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-18T16:12:52.033056Z",
     "iopub.status.busy": "2021-10-18T16:12:52.03279Z",
     "iopub.status.idle": "2021-10-18T16:19:07.772847Z",
     "shell.execute_reply": "2021-10-18T16:19:07.771951Z",
     "shell.execute_reply.started": "2021-10-18T16:12:52.033029Z"
    }
   },
   "outputs": [],
   "source": [
    "out = []\n",
    "\n",
    "embeddings = []\n",
    "ids = []\n",
    "\n",
    "\n",
    "test = False\n",
    "for filename in tqdm(sorted(glob(test_image_dir + \"/*\"))):\n",
    "\n",
    "    pil_img = Image.open(filename)\n",
    "    image = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n",
    "        prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n",
    "        generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed, temperature=1, beam_size=15)[0]\n",
    "\n",
    "    if test:\n",
    "        display(pil_img)\n",
    "        print(generated_text_prefix)\n",
    "    out.append({\n",
    "        \"id\": os.path.split(filename)[-1],\n",
    "        \"captions\": generated_text_prefix\n",
    "    })\n",
    "    \n",
    "    embeddings.append(prefix)\n",
    "    ids.append(os.path.split(filename)[-1])\n",
    "\n",
    "with open(\"predict.json\", \"w\") as f:\n",
    "    json.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
